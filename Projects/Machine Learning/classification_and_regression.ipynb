{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary packages and functions\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import breast cancer data\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Modeling\n",
    "\n",
    "## (4) Classification Method Used:\n",
    "#### K-Nearest Neighbors | Logistic Regression | Support Vector Machines | Decision Tree Classifier\n",
    "\n",
    "## Data Set Used:\n",
    "#### (Pima Indians Data set ) https://www.kaggle.com/uciml/pima-indians-diabetes-database/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data set and check for NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Preg  Glucose  BP  SkinThickness  Insulin   BMI    DPF  Age  Outcome\n",
      "0     6      148  72             35        0  33.6  0.627   50        1\n",
      "1     1       85  66             29        0  26.6  0.351   31        0\n",
      "2     8      183  64              0        0  23.3  0.672   32        1\n",
      "3     1       89  66             23       94  28.1  0.167   21        0\n",
      "4     0      137  40             35      168  43.1  2.288   33        1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read-in Pima Indians Data set\n",
    "pima = pd.read_csv('pima-indians-diabetes.csv', header= None, names= ['Preg', 'Glucose', 'BP', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age', 'Outcome'])\n",
    "\n",
    "# view head of data\n",
    "print(pima.head())\n",
    "print()\n",
    "\n",
    "# View description of data\n",
    "# print(pima.describe())\n",
    "\n",
    "# Check for NA values\n",
    "pima.isnull().any().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data set into pandas DataFrame and seperate features from target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert data set into pandas DataFrame type\n",
    "pimadf = pd.DataFrame(pima)\n",
    "\n",
    "# View data type\n",
    "# print(type(pimadf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Data: Feature Relationships | Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Scatter Plot Matrix of Feature Variables\n",
    "# sb.pairplot(pimadf[pimadf.columns[:8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of distributions\n",
    "# pimadf[pimadf.columns[:8]].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data and create a Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pimadf = pimadf.reindex(np.random.permutation(pimadf.index))\n",
    "# pimadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seperate features from target variable\n",
    "X_col_names = ['Preg', 'Glucose', 'BP', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age']\n",
    "X_pima = pimadf[X_col_names]\n",
    "y_pima = pimadf['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into train/test set; training set has a 70% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pima, y_pima,train_size= .7 , random_state=0) # Random_state set to 0 for reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Accuracy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Accuracy when choosing most frequent target variable: 0.6623376623376623\n",
      " \n",
      "Null Accuracy when choosing target variable by distribution: 0.5454545454545454\n",
      " \n",
      "Null Accuracy when choosing target variables by random: 0.5497835497835498\n"
     ]
    }
   ],
   "source": [
    "# What is our accuracy if we guess by most frequent target variable or \n",
    "# guess by the distribution of target variables or\n",
    "# guess randomly\n",
    "\n",
    "# Import Dummy Variable Classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Fit dummy classifier with data (* Most frequent)\n",
    "dummy_majority = DummyClassifier(strategy= 'most_frequent').fit(X_train, y_train)\n",
    "\n",
    "# Review score\n",
    "print(\"Null Accuracy when choosing most frequent target variable:\" + \" \" + str(dummy_majority.score(X_test, y_test)))\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "# Fit dummy classifier with data (* Target variable distribution)\n",
    "dummy_majority = DummyClassifier(strategy= 'stratified').fit(X_train, y_train)\n",
    "\n",
    "# Review score\n",
    "print(\"Null Accuracy when choosing target variable by distribution:\" + \" \" + str(dummy_majority.score(X_test, y_test)))\n",
    "print(\" \")\n",
    "\n",
    "# Fit dummy classifier with data (* Random guess)\n",
    "dummy_majority = DummyClassifier(strategy= 'uniform').fit(X_train, y_train)\n",
    "\n",
    "# Review score\n",
    "print(\"Null Accuracy when choosing target variables by random:\" + \" \" + str(dummy_majority.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.82\n",
      "Accuracy of K-NN classifier on test set: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Import KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import scaling function from sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale Dependent variables and fit to KNN classifier\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate kNN Model and fit training data\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn_predict = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How sensitive is k-NN classification accuracy to the choice of the 'k' parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_range = range(1,20)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "## Uncomment to view graph    \n",
    "# plt.figure()\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.scatter(k_range, scores)\n",
    "# plt.xticks([0,5,10,15,20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier w/ feature scaling and updated n_neighbors value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.79\n",
      "Accuracy of K-NN classifier on test set: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Set n_neighbors= 10; fit new knn to data, view accuracies\n",
    "knn = KNeighborsClassifier(n_neighbors= 10)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "knn_predict = knn.predict(X_test_scaled)\n",
    "\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation score (5-fold): 0.734\n",
      " \n",
      "Confusion Matrix\n",
      " [[139  14]\n",
      " [ 48  30]]\n",
      " \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.91      0.82       153\n",
      "          1       0.68      0.38      0.49        78\n",
      "\n",
      "avg / total       0.72      0.73      0.71       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform Confusion Matrix and 5 or 10 Fold-Cross-Validation and Classification Report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Fit Cross-Validation (CV) model with fitted classifier model and full, origional data set (X_pima & y_pima)\n",
    "cv_scores = cross_val_score(knn, X_pima, y_pima, cv= 5)\n",
    "\n",
    "print('Cross-Validation score (5-fold): {:.3f}' .format(np.mean(cv_scores)))\n",
    "print(\" \")\n",
    "print('Confusion Matrix\\n', confusion_matrix(y_test, knn.predict(X_test_scaled)))\n",
    "print(\" \")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, knn_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "#### • First, I read in the data, performed summary statistsics and checked for NA values.\n",
    "#### • Second, I scaled the data by fitting and transforming the X_train set & transforming the X_train set.\n",
    "#### • Next, I fitted the K-NN model with our training data and evaluated the training and tests set scores.\n",
    "### ° K-NN model acheived subpar classsification accuracy:\n",
    "####    Training accuracy= 77%    |  Test accuracy= 74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform another train/ test split on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into train/test set; training set has a 70% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pima, y_pima,train_size= .7 , random_state=1) # Random_state set to 1 for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier (C= 1.0, default) on training set: 0.78\n",
      "Accuracy of Logistic Regression classifier (C= 1.0, default) on test set: 0.77\n",
      " \n",
      "Accuracy of Logistic Regression classifier (C= 0.1) on training set: 0.66\n",
      "Accuracy of Logistic Regression classifier (C= 0.1) on test set: 0.66\n",
      " \n",
      "Accuracy of Logistic Regression classifier (C= 100.0) on training set: 0.78\n",
      "Accuracy of Logistic Regression classifier (C= 100.0) on test set: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Import Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import scaling function from sklearn and scale data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate Logistic Model and fit training data\n",
    "logreg = LogisticRegression(C= 1.0)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of Logistic Regression classifier (C= 1.0, default) on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Logistic Regression classifier (C= 1.0, default) on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test_scaled, y_test)))\n",
    "print(\" \")\n",
    "\n",
    "# Instantiate Logistic Model and fit training data\n",
    "logreg = LogisticRegression(C= 0.1)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of Logistic Regression classifier (C= 0.1) on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Logistic Regression classifier (C= 0.1) on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test_scaled, y_test)))\n",
    "print(\" \")\n",
    "\n",
    "# Instantiate Logistic Model and fit training data\n",
    "logreg = LogisticRegression(C= 100)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of Logistic Regression classifier (C= 100.0) on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Logistic Regression classifier (C= 100.0) on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test_scaled, y_test)))\n",
    "\n",
    "\n",
    "# logreg\n",
    "# C= 1; Train= 77, Test= 76\n",
    "# C= 0.1; Train= 67, Test= 63\n",
    "# C= 100; Train= 78, Test= 77\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "#### • First, I performed another train/ test split on the data set.\n",
    "#### • Second, I scaled the data by fitting and transforming the X_train set & transforming the X_train set\n",
    "#### • Next, I fitted the Logistic Regression model with our training data and evaluated the training and tests set scores\n",
    "#### • Finally, I tested the accuracy of the model in relationship to varying 'C' hyperperamater values.\n",
    "### ° Highest classification accuracy is acheived when hyperperamater C is set to 100:\n",
    "####    C= 1.0 (training accuracy= 77%)    |  C= 1.0 (test accuracy= 76%)\n",
    "####    C= 0.1 (training accuracy= 67%)    |  C= 0.1 (test accuracy= 63%)\n",
    "####    C= 100.0 (training accuracy= 78%)    |  C= 100.0 (test accuracy= 77%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vectore Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform another train/ test split on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train/test set; training set has a 70% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pima, y_pima,train_size= .7 , random_state=2) # Random_state set to 2 for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Machine classifier (C= 1.0, default) on training set: 0.77\n",
      "Accuracy of Support Vector Machine classifier (C= 1.0, default) on test set: 0.73\n",
      " \n",
      "Accuracy of Support Vector Machine classifier (C= 0.1, default) on training set: 0.67\n",
      "Accuracy of Support Vector Machine classifier (C= 0.1, default) on test set: 0.61\n",
      " \n",
      "Accuracy of Support Vector Machine classifier (C= 100, default) on training set: 0.79\n",
      "Accuracy of Support Vector Machine classifier (C= 100, default) on test set: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Import Support Vector Machine (SVM) Model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import scaling function from sklearn and scale data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate SVM Model and fit training data\n",
    "svm = SVC()\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of Support Vector Machine classifier (C= 1.0, default) on training set: {:.2f}'\n",
    "     .format(svm.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Support Vector Machine classifier (C= 1.0, default) on test set: {:.2f}'\n",
    "     .format(svm.score(X_test_scaled, y_test)))\n",
    "print(\" \")\n",
    "\n",
    "# Instantiate SVM Model and fit training data (different value for hyperparameter C)\n",
    "svm = SVC(C= 0.1)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of Support Vector Machine classifier (C= 0.1, default) on training set: {:.2f}'\n",
    "     .format(svm.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Support Vector Machine classifier (C= 0.1, default) on test set: {:.2f}'\n",
    "     .format(svm.score(X_test_scaled, y_test)))\n",
    "print(\" \")\n",
    "\n",
    "# Instantiate SVM Model and fit training data (different value for hyperparameter C)\n",
    "svm = SVC(C= 100)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of Support Vector Machine classifier (C= 100, default) on training set: {:.2f}'\n",
    "     .format(svm.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Support Vector Machine classifier (C= 100, default) on test set: {:.2f}'\n",
    "     .format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "#### • First, I performed another train/ test split on the data set.\n",
    "#### • Second, I scaled the data by fitting and transforming the X_train set & transforming the X_train set\n",
    "#### • Next, I fitted the Support Vector Machine model with our training data and evaluated the training and tests set scores\n",
    "#### • Finally, I tested the accuracy of the model in relationship to varying 'C' hyperperamater values.\n",
    "### ° Highest classification accuracy is acheived when hyperperamater C is set to 100:\n",
    "####    C= 1.0 (training accuracy= 75%)    |  C= 1.0 (test accuracy= 80%)\n",
    "####    C= 0.1 (training accuracy= 64%)    |  C= 0.1 (test accuracy= 68%)\n",
    "####    C= 100.0 (training accuracy= 78%)    |  C= 100.0 (test accuracy= 81%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform another train/ test split on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train/test set; training set has a 70% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pima, y_pima,train_size= .7 , random_state=3) # Random_state set to 3 for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier (max_depth= 2) on training set: 0.78\n",
      "Accuracy of K-NN classifier (max_depth= 2) on test set: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Import Decision Tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import scaling function from sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale Dependent variables and fit to KNN classifier\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate Decision Tree Model and fit training data (hyperparameter 'max_depth' set= 2)\n",
    "dtc = DecisionTreeClassifier(max_depth=2)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate training set and test set score\n",
    "print('Accuracy of K-NN classifier (max_depth= 2) on training set: {:.2f}'\n",
    "     .format(dtc.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier (max_depth= 2) on test set: {:.2f}'\n",
    "     .format(dtc.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Below is practice code \n",
    "# (Attempting to plot accuracies of different hyperperamater values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c_values = [0.1, 1.0, 100]\n",
    "# c_values_len = len(c_values)\n",
    "\n",
    "# C_range = range(0 ,c_values_len)\n",
    "# scores = []\n",
    "\n",
    "# for C in C_range:\n",
    "#     logreg = LogisticRegression(C= C)\n",
    "#     logreg.fit(X_train_scaled, y_train)\n",
    "#     scores.append(logreg.score(X_train_scaled, y_test))\n",
    "\n",
    "## Uncomment to view graph    \n",
    "# plt.figure()\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.scatter(C_range, scores)\n",
    "# plt.xticks([0,15,30,45,60,75,90,105]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#----------------------- Regression Modeling below... -----------------------------------\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Modeling\n",
    "## (3) Regression Methods Used:\n",
    "#### Linear Regression | Ridge Regression | Lasso Regression\n",
    "\n",
    "## Data Set Used:\n",
    "#### (Combined Cycle Power Plant Data Set) http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data set and check for NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ave_temp  exhaust_vacuum  ambient_pressure  relative_humidity  \\\n",
      "0      8.34           40.77           1010.84              90.01   \n",
      "1     23.64           58.49           1011.40              74.20   \n",
      "2     29.74           56.90           1007.15              41.91   \n",
      "3     19.07           49.69           1007.22              76.79   \n",
      "4     11.80           40.66           1017.13              97.20   \n",
      "\n",
      "   energy_output  \n",
      "0         480.48  \n",
      "1         445.75  \n",
      "2         438.76  \n",
      "3         453.09  \n",
      "4         464.43  \n",
      "\n",
      "Dimenion of data:(9568, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read-in Power Plant data set\n",
    "energy = pd.read_csv('Folds5x2_pp.csv', skiprows=1, names= ['ave_temp', 'exhaust_vacuum', 'ambient_pressure', 'relative_humidity', 'energy_output'])\n",
    "\n",
    "# View data head and dim\n",
    "print(energy.head())\n",
    "print(\"\")\n",
    "print('Dimenion of data:' + str(energy.shape))\n",
    "\n",
    "\n",
    "# Check for NA values\n",
    "energy.isnull().any().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View statistical description of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Results:\n",
      "All of the feature variable's values are of mixed scales. Therefore, we will scale the variables using MinMax Scale\n"
     ]
    }
   ],
   "source": [
    "# View statistical description of data set\n",
    "# print(energy.describe())\n",
    "print('')\n",
    "print('Initial Results:')\n",
    "print(\"All of the feature variable's values are of mixed scales. Therefore, we will scale the variables using MinMax Scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Visualizations* -  Distribution (Histogram) | Relationships (Scatterplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View data's distribution\n",
    "# print(energy.hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Views data's relationships between variables\n",
    "# pairs = sb.pairplot(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatterplot between energy_output and average_temp\n",
    "# scatter_TempAge = sb.regplot(x= 'energy_output', y= 'ave_temp', data= energy)\n",
    "\n",
    "# Scatterplot between exhaust vacuum and average_temp    \n",
    "# scatter_ExhaustAge = sb.regplot(x= 'exhaust vacuum', y= 'ave_temp', data= energy)\n",
    "\n",
    "# Scatterplot between relative humidity and average_temp    \n",
    "# scatter_ExhaustAge = sb.regplot(x= 'relative_humidity', y= 'ave_temp', data= energy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the two most promosing variables \n",
    "#### (* Reshape the data in order for it to work with Linear Model | ave_temp= (9568, 1) & energy_output= (9568,)* )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:13: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "# energy_output vs ave_tempt looks to have a strong relationship\n",
    "# Save energy_output & ave_tempt in their own objects \n",
    "\n",
    "ave_temp = energy[energy.columns[0]]\n",
    "energy_output = energy[energy.columns[4]]\n",
    "# ave_temp\n",
    "# energy_output\n",
    "\n",
    "\n",
    "\n",
    "## Reshape the data in order for it to work with Linear Model | ave_temp= (9568, 1) & energy_output= (9568,)\n",
    "## *Ignore any potential error messages\n",
    "ave_temp = ave_temp.reshape(-1, 1)\n",
    "# ave_temp.shape\n",
    "# energy_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Shuffle data\n",
    "# X_energy = ave_temp.reindex(np.random.permutation(ave_temp.index))\n",
    "# y_energy = energy_output.reindex(np.random.permutation(energy_output.index))\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(ave_temp, energy_output,train_size= .7, random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Accuracy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error when applying Line-of-best-fit by the mean of data: 294.96782677101066\n",
      " \n",
      "r2 score when applying Line-of-best-fit by the mean of data: -0.012572169246374276\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# What is our accuracy if we split our data by its mean \n",
    "\n",
    "# Import Dummy Variable Classifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "# Import model evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit dummy classifier with data (* Most frequent)\n",
    "dummy_majority = DummyRegressor(strategy= 'mean').fit(X_train, y_train)\n",
    "\n",
    "y_predict_dummy_mean = dummy_majority.predict(X_test)\n",
    "\n",
    "# Review Mean Squared Error\n",
    "print(\"Mean Squared Error when applying Line-of-best-fit by the mean of data:\" + \" \" + str(mean_squared_error(y_test, y_predict_dummy_mean)))\n",
    "print(\" \")\n",
    "\n",
    "# Review r2 score\n",
    "print(\"r2 score when applying Line-of-best-fit by the mean of data:\" + \" \" + str(r2_score(y_test, y_predict_dummy_mean)))\n",
    "print(\" \")\n",
    "\n",
    "# # Fit dummy classifier with data (* Target variable distribution)\n",
    "# dummy_majority = DummyClassifier(strategy= 'stratified').fit(X_train, y_train)\n",
    "\n",
    "# # Review score\n",
    "# print(\"Null Accuracy when choosing target variable by distribution:\" + \" \" + str(dummy_majority.score(X_test, y_test)))\n",
    "# print(\" \")\n",
    "\n",
    "# # Fit dummy classifier with data (* Random guess)\n",
    "# dummy_majority = DummyClassifier(strategy= 'uniform').fit(X_train, y_train)\n",
    "\n",
    "# # Review score\n",
    "# print(\"Null Accuracy when choosing target variables by random:\" + \" \" + str(dummy_majority.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression *WITHOUT* feature scaling\n",
    "#### *Only using the variables ave_temp and energy_output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy dataset\n",
      "linear regression linear model intercept: 257.0218157243215\n",
      "linear regression linear model coeff:\n",
      "[-1.61063462 -0.38009644  0.25642869 -0.13453288]\n",
      " \n",
      "R-squared score (test): 0.922\n",
      "Mean Squared Error score (test): 22.739\n",
      " \n",
      "Number of non-zero features: 4\n"
     ]
    }
   ],
   "source": [
    "# Import Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit scaled data to linear model\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "y_predict_linreg = linreg.predict(X_test)\n",
    "\n",
    "print('Energy dataset')\n",
    "print('linear regression linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear regression linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print(\" \")\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(r2_score(y_test, y_predict_linreg)))\n",
    "print('Mean Squared Error score (test): {:.3f}'\n",
    "     .format(mean_squared_error(y_test, y_predict_linreg)))\n",
    "print(\" \")\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linreg.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression *WITH*  feature scaling\n",
    "#### *Only using the variables ave_temp and energy_output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale data feature values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy dataset\n",
      "linear regression linear model intercept: 493.16265635905836\n",
      "linear regression linear model coeff:\n",
      "[-76.76543094]\n",
      "R-squared score (training): 0.898\n",
      "R-squared score (test): 0.901\n",
      "Number of non-zero features: 1\n"
     ]
    }
   ],
   "source": [
    "# Import Regression model\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit scaled data to linear model\n",
    "linreg = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Energy dataset')\n",
    "print('linear regression linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear regression linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linreg.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "#### • First, I read in the data set, checked for NA values, and did descriptive statistics.\n",
    "#### • Second, I visualized the data ( Scatterplot & Histogram)\n",
    "#### • Next, I took the variable (ave_tempt) that had a strong negative relationship w/ or target variable and save it in its own object\n",
    "#### • I performed a train/test split on the data and fit to a Linear Regression Model\n",
    "### ° ave_tempt showed to be a decent predictor for energy_output:\n",
    "####   -  R-squared score (training): 90%   |  R-squared score (test): 90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is work-in-progress\n",
    "## I believe the following code is to be used with Ridge regression and/or 'logistic' regression\n",
    "#### estimatedCoefficients of each feature variable compared to 'outcome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = pd.DataFrame(ave_temp.columns, columns= ['features'])\n",
    "# # a\n",
    "# a[\"estimatedCoefficients\"] = linreg.coef_\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View origional data set\n",
    "# energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate features from target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seperate features from target variable\n",
    "X_col_names = ['ave_temp', 'exhaust_vacuum', 'ambient_pressure', 'relative_humidity']\n",
    "X_energy = energy[X_col_names]\n",
    "y_energy = energy['energy_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_energy, y_energy, train_size= 70, random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Ridge Regression using all feature variables | feature values scaled*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy dataset\n",
      " \n",
      "ridge regression linear model intercept: 457.70969440850405\n",
      "ridge regression linear model coeff:\n",
      "[-9.06965079 -9.51838016  4.70408301  4.50077641]\n",
      " \n",
      "R-squared score (test): 0.555\n",
      "0.5551450441564407\n",
      "MeanSquaredError score (test): 129.589\n",
      " \n",
      "Number of non-zero features: 4\n"
     ]
    }
   ],
   "source": [
    "# For Ridge Regression, feature scaling/ normalizing is necessary \n",
    "# Therefore, we will normalize the data set before performing Ridge Regression\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Import MinMaxScaler and normalize data set\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Import Ridge Regression Model and fit scaled data to model \n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "linridge = Ridge(alpha= 20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "y_predict_linridge = linridge.predict(X_test_scaled)\n",
    "\n",
    "# View training set and test set accuracies\n",
    "print('Energy dataset')\n",
    "print(\" \")\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print(\" \")\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(r2_score(y_test, y_predict_linridge)))\n",
    "print('MeanSquaredError score (test): {:.3f}'\n",
    "     .format(mean_squared_error(y_test, y_predict_linridge)))\n",
    "print(\" \")\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))\n",
    "\n",
    "\n",
    "\n",
    "# print('R-squared score (test): {:.3f}'\n",
    "#      .format(linridge.score(X_train_scaled, y_train)))\n",
    "# print('R-squared score (test): {:.3f}'\n",
    "#      .format(linridge.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative GridSearch Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162179415120615\n",
      "Ridge(alpha=0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "# Import GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "linridge = Ridge()\n",
    "grid_values = {'alpha': [0, 1, 10, 20, 50, 100, 1000]}\n",
    "\n",
    "grid_linridge_acc = GridSearchCV(linridge, param_grid= grid_values)\n",
    "grid_linridge_acc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best paramater value and its accuracy\n",
    "print(grid_linridge_acc.best_score_)\n",
    "print(grid_linridge_acc.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-run Ridge Regression with Alpha value that achieves highest r-squared score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy dataset\n",
      " \n",
      "ridge regression linear model intercept: 484.7146911030029\n",
      "ridge regression linear model coeff:\n",
      "[-43.92200614 -14.9073823    7.40822499  -7.75716593]\n",
      " \n",
      "R-squared score (train): 0.935\n",
      "R-squared score (test): 0.922\n",
      "MeanSquaredError score (test): 22.739\n",
      " \n",
      "Number of non-zero features: 4\n"
     ]
    }
   ],
   "source": [
    "linridge = Ridge(alpha= 0.0).fit(X_train_scaled, y_train)\n",
    "y_predict_linridge = linridge.predict(X_test_scaled)\n",
    "\n",
    "# View training set and test set accuracies\n",
    "print('Energy dataset')\n",
    "print(\" \")\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print(\" \")\n",
    "print('R-squared score (train): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('MeanSquaredError score (test): {:.3f}'\n",
    "     .format(mean_squared_error(y_test, y_predict_linridge)))\n",
    "print(\" \")\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "#### • First, I seperated feature variables from target variable\n",
    "#### • Second, I performed a train/test split on the data and fit to a Ridge Regression Model | hyperperamater 'Alpha' set= 20\n",
    "### ° Initial results- train/ test set accuracies are lower than Linear Model's accuracies:\n",
    "####   -  R-squared score (training): 60%   |  R-squared score (test): 55%\n",
    "#### • Next, I examined the model's accuracies with varying values for hyperperamater 'Alpha'\n",
    "#### •  An 'Alpha' value of 0 showed to achieve highest accuracy | Re-run model with Alpha= 0\n",
    "### ° Secondary Results- With hyperperameter set= 0, Ridge Regression achieved promising accuracies:\n",
    "#### -  R-squared score (training): 93%   |  R-squared score (test): 92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View origional data set\n",
    "# energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate features from target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperate features from target variable\n",
    "X_col_names = ['ave_temp', 'exhaust_vacuum', 'ambient_pressure', 'relative_humidity']\n",
    "X_energy = energy[X_col_names]\n",
    "y_energy = energy['energy_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_energy, y_energy, train_size= 70, random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Lasso Regression using all feature variables | feature values scaled*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy dataset\n",
      " \n",
      "ridge regression linear model intercept: 479.87677209703963\n",
      "ridge regression linear model coeff:\n",
      "[-33.27633412 -17.32755484   0.           0.        ]\n",
      " \n",
      "R-squared score (train): 0.908\n",
      "R-squared score (test): 0.894\n",
      "MeanSquaredError score (test): 30.981\n",
      " \n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tave_temp, -33.276\n",
      "\texhaust_vacuum, -17.328\n"
     ]
    }
   ],
   "source": [
    "# For Lasso Regression, feature scaling/ normalizing is necessary \n",
    "# Therefore, we will normalize the data set before performing Ridge Regression\n",
    "\n",
    "# Import MinMaxScaler and normalize data set\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Import Ridge Regression Model and fit scaled data to model \n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "linlasso = Lasso(alpha= 0.5, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "y_predict_linlasso = linlasso.predict(X_test_scaled)\n",
    "\n",
    "print('Energy dataset')\n",
    "print(\" \")\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print(\" \")\n",
    "print('R-squared score (train): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('MeanSquaredError score (test): {:.3f}'\n",
    "     .format(mean_squared_error(y_test, y_predict_linlasso)))\n",
    "print(\" \")\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_energy), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "#### • First, I seperated feature variables from target variable\n",
    "#### • Second, I performed a train/test split on the data and fit to a Lasso Regression Model | hyperperamater 'Alpha' set= 1\n",
    "### ° Initial results- train/ test set accuracies are higher than Linear Regression but lower than Ridge Model's accuracies:\n",
    "####   -  R-squared score (training): 90%   |  R-squared score (test): 89%\n",
    "####   - (2) feature variables showed to have non-zero values for our model: ave_temp & exhaust_vacuum\n",
    "####   - Both non-zero features have a negative correlation with our dependent variable energy_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model coeff (w): [-1.98357941 -0.23219575  0.06559288 -0.15932893]\n",
      "linear model intercept (b): 451.191\n",
      " \n",
      "R-squared score (test): 0.929\n",
      "MeanSquaredError score (test): 20.544\n",
      "\n",
      "Now we transform the original input data to add\n",
      "polynomial features up to degree 2 (quadratic)\n",
      "\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 0.00000000e+00 -5.86827173e+00 -2.95152189e+00  1.56380646e+01\n",
      "  4.09323406e+00  1.59324371e-02  1.26142482e-02  3.22997588e-03\n",
      " -7.06194295e-03 -1.36728979e-03  2.45140439e-03  9.48825801e-04\n",
      " -7.61221143e-03 -3.76971757e-03 -2.10743929e-03]\n",
      "(poly deg 2) linear model intercept (b): -7518.264\n",
      " \n",
      "(poly deg 2) R-squared score (train): 0.938\n",
      "(poly deg 2) R-squared score (test): 0.938\n",
      "(poly deg 2) MeanSquaredError score (test): 18.034\n",
      "\n",
      "\n",
      "Addition of many polynomial features often leads to\n",
      "overfitting, so we often use polynomial features in combination\n",
      "with regression that has a regularization penalty, like ridge\n",
      "regression.\n",
      "\n",
      "(poly deg 2 + ridge) linear model coeff (w):\n",
      "[ 0.00000000e+00 -6.08230870e+00 -3.46697236e+00  1.19894992e+01\n",
      "  3.85307768e+00  1.55804751e-02  1.30064461e-02  3.42964945e-03\n",
      " -7.00061148e-03 -1.45511825e-03  2.96051711e-03  9.86465858e-04\n",
      " -5.83570584e-03 -3.53867399e-03 -2.08907680e-03]\n",
      "(poly deg 2 + ridge) linear model intercept (b): -5644.998\n",
      " \n",
      "(poly deg 2 + ridge) R-squared score (train): 0.938\n",
      "(poly deg 2) R-squared score (test): 0.938\n",
      "(poly deg 2 + ridge) MeanSqaureError score (test): 18.040\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_energy, y_energy, random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print(\" \")\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "print('MeanSquaredError score (test): {:.3f}'\n",
    "     .format(mean_squared_error(y_test, linreg.predict(X_test))))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_energy_poly = poly.fit_transform(X_energy)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_energy_poly, y_energy, random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "y_predict_linreg = linreg.predict(X_test)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print(\" \")\n",
    "print('(poly deg 2) R-squared score (train): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "print('(poly deg 2) MeanSquaredError score (test): {:.3f}\\n'\n",
    "     .format(mean_squared_error(y_test, y_predict_linreg)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_energy_poly, y_energy, random_state = 0)\n",
    "linridge = Ridge().fit(X_train, y_train)\n",
    "y_predict_linridge = linridge.predict(X_test)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linridge.intercept_))\n",
    "print(\" \")\n",
    "print('(poly deg 2 + ridge) R-squared score (train): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('(poly deg 2 + ridge) MeanSqaureError score (test): {:.3f}'\n",
    "     .format(mean_squared_error(y_test, y_predict_linridge)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, I will perform Predictions on test set to see how well our models\n",
    "# generalize to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
